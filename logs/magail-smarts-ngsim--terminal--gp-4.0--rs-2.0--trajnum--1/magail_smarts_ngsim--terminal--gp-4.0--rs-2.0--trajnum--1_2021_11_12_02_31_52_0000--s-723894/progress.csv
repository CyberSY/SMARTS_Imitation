policy_0 Disc CE Loss,policy_0 Disc Acc,policy_0 Grad Pen,policy_0 Grad Pen W,agent_0 Disc Rew Mean,agent_0 Disc Rew Std,agent_0 Disc Rew Max,agent_0 Disc Rew Min,agent_1 Disc Rew Mean,agent_1 Disc Rew Std,agent_1 Disc Rew Max,agent_1 Disc Rew Min,policy_0 Reward Scale,policy_0 QF1 Loss,policy_0 QF2 Loss,policy_0 Alpha Loss,policy_0 Policy Loss,policy_0 Q1 Predictions Mean,policy_0 Q1 Predictions Std,policy_0 Q1 Predictions Max,policy_0 Q1 Predictions Min,policy_0 Q2 Predictions Mean,policy_0 Q2 Predictions Std,policy_0 Q2 Predictions Max,policy_0 Q2 Predictions Min,policy_0 Alpha Mean,policy_0 Alpha Std,policy_0 Alpha Max,policy_0 Alpha Min,policy_0 Log Pis Mean,policy_0 Log Pis Std,policy_0 Log Pis Max,policy_0 Log Pis Min,policy_0 Policy mu Mean,policy_0 Policy mu Std,policy_0 Policy mu Max,policy_0 Policy mu Min,policy_0 Policy log std Mean,policy_0 Policy log std Std,policy_0 Policy log std Max,policy_0 Policy log std Min,Test agent_0 Success Rate,Test agent_0 Collision Rate,Test agent_0 Distance Mean,Test agent_0 Distance Std,Test agent_0 Distance Max,Test agent_0 Distance Min,Test agent_0 Rewards Mean,Test agent_0 Rewards Std,Test agent_0 Rewards Max,Test agent_0 Rewards Min,Test agent_0 Returns Mean,Test agent_0 Returns Std,Test agent_0 Returns Max,Test agent_0 Returns Min,Test agent_0 Actions Mean,Test agent_0 Actions Std,Test agent_0 Actions Max,Test agent_0 Actions Min,Test agent_1 Success Rate,Test agent_1 Collision Rate,Test agent_1 Distance Mean,Test agent_1 Distance Std,Test agent_1 Distance Max,Test agent_1 Distance Min,Test agent_1 Rewards Mean,Test agent_1 Rewards Std,Test agent_1 Rewards Max,Test agent_1 Rewards Min,Test agent_1 Returns Mean,Test agent_1 Returns Std,Test agent_1 Returns Max,Test agent_1 Returns Min,Test agent_1 Actions Mean,Test agent_1 Actions Std,Test agent_1 Actions Max,Test agent_1 Actions Min,Test Ep. Len. Mean,Test Ep. Len. Std,Test Ep. Len. Max,Test Ep. Len. Min,Num Paths,Exploration agent_0 Success Rate,Exploration agent_0 Collision Rate,Exploration agent_0 Distance Mean,Exploration agent_0 Distance Std,Exploration agent_0 Distance Max,Exploration agent_0 Distance Min,Exploration agent_0 Rewards Mean,Exploration agent_0 Rewards Std,Exploration agent_0 Rewards Max,Exploration agent_0 Rewards Min,Exploration agent_0 Returns Mean,Exploration agent_0 Returns Std,Exploration agent_0 Returns Max,Exploration agent_0 Returns Min,Exploration agent_0 Actions Mean,Exploration agent_0 Actions Std,Exploration agent_0 Actions Max,Exploration agent_0 Actions Min,Exploration agent_1 Success Rate,Exploration agent_1 Collision Rate,Exploration agent_1 Distance Mean,Exploration agent_1 Distance Std,Exploration agent_1 Distance Max,Exploration agent_1 Distance Min,Exploration agent_1 Rewards Mean,Exploration agent_1 Rewards Std,Exploration agent_1 Rewards Max,Exploration agent_1 Rewards Min,Exploration agent_1 Returns Mean,Exploration agent_1 Returns Std,Exploration agent_1 Returns Max,Exploration agent_1 Returns Min,Exploration agent_1 Actions Mean,Exploration agent_1 Actions Std,Exploration agent_1 Actions Max,Exploration agent_1 Actions Min,Exploration Ep. Len. Mean,Exploration Ep. Len. Std,Exploration Ep. Len. Max,Exploration Ep. Len. Min,AgentMeanAverageReturn,Number of train calls total,Number of env steps total,Number of rollouts total,Train Time (s),(Previous) Eval Time (s),Sample Time (s),Epoch Time (s),Total Train Time (s),Epoch
0.6984133,0.375,0.9638883,4.0,0.17774053,0.379298,2.8836646,0.0039061492,0.31079024,0.38154536,1.7733771,0.008711873,2.0,1.080196,0.9711007,-3.9060237,-0.24800073,0.025532851,0.015069857,0.057202514,0.0004900122,0.10270282,0.042347543,0.18151435,0.034006696,0.19994000899934725,0.0,0.19994000899934725,0.19994000899934725,-0.42694896,0.2881873,0.31710625,-0.9146806,-0.021693468,0.031765226,0.020422656,-0.09138516,-0.0002999999,2.0579516e-11,-0.00029999987,-0.0002999999,0.0,0.0,66.25697096804744,25.162567833511016,108.99156310825572,22.223485178200242,0.483646878866274,0.5471643897289513,3.886702885829763,0.0,74.63043376967273,29.573094873765143,125.00818259328794,22.223345032049235,0.06711411,0.05541299,0.25983867,0.0002007191,0.0,0.0,81.87925989442041,33.74892926571378,137.291766572772,31.381009967815018,0.5210416553244223,0.5597677156448109,3.8611393864709385,0.0,92.82557490241246,37.81563215619478,152.8331414095738,31.97579128147318,0.065017514,0.057884548,0.21067232,-0.006033687,154.30769230769232,54.36540842425535,309.0,98.0,1.0,0.0,0.0,140.22718881823133,0.0,140.22718881823133,140.22718881823133,0.3912607236293478,0.5069881354977105,3.8328827691960754,0.0,155.72176800448042,0.0,155.72176800448042,155.72176800448042,-0.03173169,0.6224772,0.99403244,-0.9987486,0.0,0.0,208.29302299860728,0.0,208.29302299860728,208.29302299860728,0.4493308243922611,0.44847154977624165,3.7466367886875673,0.0,223.31741972295376,0.0,223.31741972295376,223.31741972295376,-0.022793094,0.63919014,0.99584955,-0.99482405,398.0,0.0,398.0,398.0,41.864002168021294,1,1100,2,2.137294645421207,7.133418694138527e-05,1.123505254741758,3.2608712343499064,51.93077362095937,10
