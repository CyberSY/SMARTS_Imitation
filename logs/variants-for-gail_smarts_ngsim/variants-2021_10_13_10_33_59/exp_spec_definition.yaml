constants:
  adv_irl_params:
    disc_lr: 0.0003
    disc_momentum: 0.9
    disc_optim_batch_size: 256
    eval_deterministic: true
    eval_no_terminal: false
    expert_buffer_size: 1000000
    freq_saving: 20
    max_path_length: 1000
    min_steps_before_training: 5000
    mode: gail
    no_terminal: false
    num_disc_updates_per_loop_iter: 1
    num_epochs: 162
    num_policy_updates_per_loop_iter: 1
    num_steps_between_train_calls: 1000
    num_steps_per_epoch: 100000
    num_steps_per_eval: 20000
    num_update_loops_per_train_call: 100
    policy_optim_batch_size: 256
    policy_optim_batch_size_from_expert: 0
    replay_buffer_size: 2000000
    save_algorithm: false
    save_best: true
    save_environment: false
    save_epoch: false
    save_replay_buffer: false
    state_only: true
    use_grad_pen: true
    wrap_absorbing: false
  disc_clamp_magnitude: 10.0
  disc_hid_act: tanh
  disc_hid_dim: 128
  disc_num_blocks: 2
  disc_use_bn: false
  env_specs:
    env_creator: smarts
    env_kwargs: {}
    eval_env_num: 1
    eval_env_seed: 78236
    scenario_name: ngsim
    training_env_num: 2
    training_env_seed: 24495
  expert_idx: 0
  expert_name: smarts_ngsim
  minmax_env_with_demo_stats: false
  policy_net_size: 256
  policy_num_hidden_layers: 2
  sac_params:
    alpha: 0.2
    beta_1: 0.25
    discount: 0.99
    policy_lr: 0.0003
    policy_mean_reg_weight: 0.001
    policy_std_reg_weight: 0.001
    qf_lr: 0.0003
    soft_target_tau: 0.005
    vf_lr: 0.0003
  scale_env_with_demo_stats: false
  traj_num: -1
meta_data:
  description: Train an adversarial IRL model
  exp_name: gail_smarts_ngsim
  num_workers: 2
  script_path: run_scripts/adv_irl_exp_script.py
  using_gpus: true
variables:
  adv_irl_params:
    grad_pen_weight:
    - 4.0
  sac_params:
    reward_scale:
    - 2.0
  seed:
  - 723894
